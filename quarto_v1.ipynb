{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2_p6cn1BoM3y"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thiziri-Hafir/Donnees_Massives/blob/main/quarto_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "title: \"Projet : Changement de production de café en fonction du climat au Brésil\"\n",
        "format:\n",
        "  html:\n",
        "    code-fold: true\n",
        "jupyter: python3\n",
        "---"
      ],
      "metadata": {
        "id": "yV5ppd0aqC49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "a1k8EAtnhX45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "HWInYt5nT7vZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the data"
      ],
      "metadata": {
        "id": "zVBfQxoCVXdB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nB1xk38dTrBN"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "import pandas as pd\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "SparkContext.setSystemProperty('spark.executor.memory', '8g')\n",
        "SparkContext.setSystemProperty('spark.driver.memory', '45G')\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "spark = SparkSession.builder.appName(\"Python Spark\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/jldbc/coffee-quality-database/master/data/arabica_data_cleaned.csv\"\n",
        "raw_arabica = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "l9lkq3ObU7gE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# Auxiliar functions\n",
        "\n",
        "def count_missings(spark_df,sort=True):\n",
        "    \"\"\"\n",
        "    Counts number of nulls and nans in each column\n",
        "    \"\"\"\n",
        "    df = spark_df.select([F.count(F.when(F.isnan(c) | F.isnull(c), c)).alias(c) for (c,c_type) in spark_df.dtypes if c_type not in ('timestamp', 'string', 'date')]).toPandas()\n",
        "\n",
        "    if len(df) == 0:\n",
        "        print(\"There are no any missing values!\")\n",
        "        return None\n",
        "\n",
        "    if sort:\n",
        "        return df.rename(index={0: 'count'}).T.sort_values(\"count\",ascending=False)\n",
        "\n",
        "    return df\n",
        "\n",
        "def equivalent_type(f):\n",
        "    \"\"\" \n",
        "         Function that changes the input Python type to PySpark type\n",
        "    \"\"\"\n",
        "    if f == 'datetime64[ns]': return TimestampType()\n",
        "    elif f == 'int64': return LongType()\n",
        "    elif f == 'int32': return IntegerType()\n",
        "    elif f == 'float64': return DoubleType()\n",
        "    elif f == 'float32': return FloatType()\n",
        "    else: return StringType()\n",
        "\n",
        "def define_structure(string, format_type):\n",
        "    \"\"\" \n",
        "         Function that define the structure of the input\n",
        "    \"\"\"\n",
        "    try: typo = equivalent_type(format_type)\n",
        "    except: typo = StringType()\n",
        "    return StructField(string, typo)\n",
        "\n",
        "# Given pandas dataframe, it will return a spark's dataframe.\n",
        "def pandas_to_spark(pandas_df):\n",
        "    \"\"\" \n",
        "         Function that transforms an input pandas dataframe to a spark dataframe\n",
        "    \"\"\"\n",
        "    columns = list(pandas_df.columns)\n",
        "    types = list(pandas_df.dtypes)\n",
        "    struct_list = []\n",
        "    for column, typo in zip(columns, types): \n",
        "      struct_list.append(define_structure(column, typo))\n",
        "    p_schema = StructType(struct_list)\n",
        "    return spark.createDataFrame(pandas_df, p_schema)"
      ],
      "metadata": {
        "id": "JT1QNG7yU_S3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creaate a spark dataframe from our initial pandas dataframe \n",
        "\n",
        "raw_arabica[['Variety']]= raw_arabica[['Variety']].fillna('unknown')\n",
        "df_arabica = pandas_to_spark(raw_arabica)  \n",
        "df_arabica.show(10)"
      ],
      "metadata": {
        "id": "u2Gds8tjVCkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic structural transformations and harmonization"
      ],
      "metadata": {
        "id": "x_-mzrIgVcQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Renaming columns and selecting the columns and rows of interest"
      ],
      "metadata": {
        "id": "KE1RpgPaV8YR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In the next cell, we rename the columns to facilitate their use.\n"
      ],
      "metadata": {
        "id": "v-w3xAVODkKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tempList = [] #Edit01\n",
        "for col in df_arabica.columns:\n",
        "        new_name = col.strip()\n",
        "        new_name = \"\".join(new_name.split())\n",
        "        new_name = new_name.replace('.','_') # EDIT\n",
        "        tempList.append(new_name) #Edit02\n",
        "print(tempList) #Just for the sake of it #Edit03\n",
        "\n",
        "df_arabica = df_arabica.toDF(*tempList) #Edit04"
      ],
      "metadata": {
        "id": "42Ntf6-3VE90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_missings(df_arabica)"
      ],
      "metadata": {
        "id": "LqSIP8nIoH-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We decided to focus on Brazil as it is one of the biggest coffee producer and presents interesting geoclimatic features and diversity"
      ],
      "metadata": {
        "id": "VcAZCNvSYQsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_arabica = df_arabica.filter(df_arabica[\"Country_of_Origin\"]=='Brazil')\\\n",
        "          .distinct()\\\n",
        "          .select([\"Farm_name\",\"Altitude\",\"Variety\",\"Aroma\",\"Flavor\",\"Aftertaste\",\\\n",
        "                   \"Acidity\",\"Body\",\"Balance\",\"Uniformity\",\"Clean_Cup\",\"Sweetness\",\"Total_Cup_Points\",\"altitude_mean_meters\"])\n",
        "df_arabica.show(100, 100)"
      ],
      "metadata": {
        "id": "8bupiicSVOGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import regexp_replace\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_arabica = df_arabica.withColumn('Farm_name', regexp_replace(col('Farm_name'), \"/\", \"_\"))"
      ],
      "metadata": {
        "id": "aoyerjXoe_ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We drop all rows with where altitude_mean_meters==NaN\n",
        "df_arabica = df_arabica.na.drop(subset=[\"altitude_mean_meters\"]) "
      ],
      "metadata": {
        "id": "aYw5eX0GDYOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_arabica.show(10,10)"
      ],
      "metadata": {
        "id": "wYlLw3A0MNHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### At first sight, we can notice that we need to harmonize the Altitude column values and link each Farm_name to its position.\n"
      ],
      "metadata": {
        "id": "wvMFtbhxW7PG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Harmonizing the altitude column"
      ],
      "metadata": {
        "id": "iLx9HyzwXRto"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HZIYqUlFXY7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linking Farm_names to actual longitudes and latitudes"
      ],
      "metadata": {
        "id": "cCoucEK9Xgxz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " #### We created by hand a dataset with the position of the fields of each Farm_name and performed a simple outer_join"
      ],
      "metadata": {
        "id": "9Vz3Us9hXmN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_farms = spark.createDataFrame(\n",
        "    [\n",
        "        (\"fazenda rio verde\",-21.877600079894428, -45.17833587173798 ), \n",
        "        (\"fazenda do lobo\", -20.05845833645814, -45.551377369807916),\n",
        "        (\"fazenda grota funda\",-4.498676298642221, -46.01438009152326),\n",
        "        (\"sitio claro\",-12.404400112670487, -57.0307320652044),\n",
        "        (\"santa alina\",-21.76121880758086, -46.674253002625804),\n",
        "        (\"fazenda chamusca\",-21.4532483711391, -45.22708818550831),\n",
        "        (\"santa maria\",-16.609194745742165, -46.98365752985455),\n",
        "        (\"capoeirinha\",-18.64698614414741, -45.796849953979795),\n",
        "        (\"fazenda do sertao\",-22.09933939187727, -45.18968007378277),\n",
        "        (\"santa fé 2\",-17.582654936016926, -47.2198752950817),\n",
        "        (\"café do paraíso\",-22.094568087843687, -45.155496432162685),\n",
        "        (\"cachoeira da grama farm\",-21.76626556295618, -46.702161544954144),\n",
        "        (\"são francisco da serra\",-22.629913000779446, -44.601043902591805),\n",
        "        (\"fazenda jericó\",-18.676315608302055, -45.70281563093488),\n",
        "        (\"sertao farm\",-22.099418916815903, -45.18965861611114),\n",
        "        (\"campo das flores\",-20.312258124962906, -43.28548394936318),\n",
        "        (\"olhos d'agua\",-18.63831446703355, -46.952789277527444),\n",
        "        (\"fazenda serra de três barras\",-19.560780988980817, -46.579303497451384),\n",
        "        (\"fazenda pantano\",-18.631997524421426, -46.82473127385452),\n",
        "        (\"pereira estate coffee\",-22.112480964214626, -45.15508902245408),\n",
        "        (\"rio verde\",-21.940510105475326, -45.176192869009476),\n",
        "        (\"sitío são geraldo\",-22.59528143237296, -46.66759241371165),\n",
        "        (\"fazenda baipendi\",-21.45008093579659, -46.8357381855336),\n",
        "        (\"água limpa\",-21.44645605895183, -46.82635044153258),\n",
        "        (\"fazenda kaquend\",-21.435236392716707, -46.83244978479159),\n",
        "        (\"fazenda santo antonio\",-21.40862617568116, -46.80560442679252),\n",
        "        (\"fazenda vista alegre\",-21.44237673402853, -46.818513026788274),\n",
        "        (\"fazenda recreio\",-21.780100162105256, -46.67880344388111),\n",
        "        (\"fazenda capoeirnha\",-21.761009557830032, -46.67507912298348),\n",
        "        (\"pantano\",-21.444638601462618, -46.81747769417689),\n",
        "        (\"fazenda são sebastião\",-21.444863288363614, -46.827235570452125),\n",
        "        (\"santa bárbara\",-18.52732198399872, -47.569893903127564),\n",
        "        (\"santa mariana\",-23.19046310851501, -50.55948683365515),\n",
        "        (\"sertao\",-22.095760723175886, -45.189755175672296),\n",
        "        (\"são rafael_ ra/ras certified\",-22.784292492565246, -47.032534461666536),\n",
        "        (\"sitío santa luzia\",-22.366016295045306, -46.47371725096838),\n",
        "        (\"fazenda são josé mirante\",-22.785019797545694, -47.03177457322967),\n",
        "        (\"cianorte\",-22.774138613223663, -47.02968245276757),\n",
        "        (\"juliana\",-21.46270024286904, -46.832515362350975),\n",
        "        (\"sitío corrego da olaria/são caetano\",-22.745464486282955, -47.0338567362583),\n",
        "        (\"fazenda serra negra\",-21., -46.674253002625804),\n",
        "        (\"fazendas klem ltda\",-20.27591897959779, -41.876604035124465),\n",
        "        (\"castelhana farm\",-18.922960557030457, -47.45831617257145),\n",
        "        (\"leticia farm\",-19.807319369835636, -42.215697182335234),\n",
        "        (\"helena\",-22.107657064931225, -48.32067672583973),\n",
        "        (\"caxambu\",-21.339808722609096, -45.42201735838091),\n",
        "    ],\n",
        "    [\"Farm_name\", \"lat\",\"long\"]  # we add column names here\n",
        ")"
      ],
      "metadata": {
        "id": "OEVstVcWXvKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Joining the two dataframes to made one \n",
        "df_arabica = df_arabica.join(df_farms, on=['Farm_name'], how='left_outer')"
      ],
      "metadata": {
        "id": "yfpoxlabXwzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_arabica.show(10)"
      ],
      "metadata": {
        "id": "kdU0wnUtYjcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We then filter the coffee to keep only those whose origin is known (Farm_name)."
      ],
      "metadata": {
        "id": "U4tG1Tj4aSMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_arabica = df_arabica.filter(df_arabica.Farm_name != \"NaN\")\n",
        "df_arabica.show(10)"
      ],
      "metadata": {
        "id": "LNX4DwUXahX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We also replace NaN values in the Variety column by \"Unknonwn\" as the information about the variety is missing."
      ],
      "metadata": {
        "id": "VHm_XPDjJQGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_arabica = df_arabica.replace(float('nan'), None)\n",
        "\n",
        "df_arabica.show(10)"
      ],
      "metadata": {
        "id": "8lOqDuy2alSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Exploration and plotting"
      ],
      "metadata": {
        "id": "B56DhNgJYtod"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Now that we cleaned and structured the data, we will explore them to fully understand them and get more details abour their behavour. "
      ],
      "metadata": {
        "id": "LPxQfwvLNF3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Location of coffee farms "
      ],
      "metadata": {
        "id": "9NSDdvHxMyNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### On the map below, we can see the projection of the farms that produce coffee in Brazil. We can see at first sight that they are mainly in the south east of the country, on the side of the Atlantic Ocean. This gives us a first idea of the meteorological conditions necessary for this production,since the south-east is a region with a subtropical climate. The summer can be very hot, even stifling, and there are many tropical rains, as sudden as they can be violent, especially in Rio and São Paulo. Winter, from June to September, is rather mild, with temperatures between 12 and 25 degrees."
      ],
      "metadata": {
        "id": "EjPjNbIiKIsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "fig = px.scatter_mapbox(df_arabica.toPandas(), lat=\"lat\", lon=\"long\", hover_name=\"Farm_name\", hover_data=[\"altitude_mean_meters\",], color_discrete_sequence=[\"fuchsia\"], zoom=3, height=300)\n",
        "fig.update_layout(mapbox_style=\"open-street-map\")\n",
        "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "_rXD9XPk82c1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The correlation between the different taste of coffee variables "
      ],
      "metadata": {
        "id": "T5e3HvTNMIpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_arabica.groupBy('Variety')\\\n",
        "          .count().alias(\"count\")\\\n",
        "          .show()"
      ],
      "metadata": {
        "id": "GAwfXpiPOuNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the Seaborn library, we plot the individuals on a dual axis to see if there are overall correlations between the taste variables (aroma, flavor, aftertaste, acidity, sweetness). We can also observe if individuals of the same variety share the same correlation on this axis. Finally we can observe how the varieties are distributed on the correlation axis."
      ],
      "metadata": {
        "id": "_kAws9vtG9tY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = sns.PairGrid(df_arabica.toPandas(), vars=['Aroma','Flavor','Aftertaste','Acidity','Sweetness'],\n",
        "                 hue='Variety', palette='RdBu_r')\n",
        "g.map(plt.scatter, alpha=0.4)\n",
        "g.add_legend()"
      ],
      "metadata": {
        "id": "VVIjr9GzUJgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternative visualisation"
      ],
      "metadata": {
        "id": "PJ-WA6oTNXZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install seaborn --upgrade \n",
        "#if there are errors : upgrade seaborn then restart kernel"
      ],
      "metadata": {
        "id": "F1opM4DDV2H0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = sns.PairGrid(df_arabica.toPandas(), vars=['Aroma','Flavor','Aftertaste','Acidity','Sweetness'], hue='Variety', palette = \"deep\")\n",
        "g.map_upper(sns.scatterplot)\n",
        "g.map_lower(sns.kdeplot, warn_singular=False)\n",
        "g.map_diag(sns.kdeplot, warn_singular=False, lw = 3)\n",
        "g.add_legend()"
      ],
      "metadata": {
        "id": "OnmuuuC_-38x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We may need more individuals to find a clear pattern."
      ],
      "metadata": {
        "id": "hXC0kVGzSp6d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Taste profile and scoring relation "
      ],
      "metadata": {
        "id": "DVAzLN2GYxVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In the section below, we will see how the different tastes of coffee influence the rating of Total_Cup_Score"
      ],
      "metadata": {
        "id": "xnwnNbS-bfIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating the features vector which are : Aroma, Flavor, Aftertaste, Acidity, Body, Balance, Uniformity, Clean_Cup, Sweetnes."
      ],
      "metadata": {
        "id": "pCXx2cCUdTVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "vectorAssembler = VectorAssembler(inputCols = [\"Aroma\",\"Flavor\",\"Aftertaste\",\\\n",
        "                   \"Acidity\",\"Body\", \"Balance\", \"Clean_Cup\", \"Uniformity\",\"Sweetness\"], outputCol = 'features')\n",
        "vec_df_arabica = vectorAssembler.transform(df_arabica)"
      ],
      "metadata": {
        "id": "9B70pLp0bd6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train test splitting"
      ],
      "metadata": {
        "id": "MeVxFFA5dQaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_taste,test_taste  = vec_df_arabica.randomSplit([0.7, 0.3])"
      ],
      "metadata": {
        "id": "8Dnb45XwdPoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### LR model"
      ],
      "metadata": {
        "id": "ZDjMHxfXdj1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "lr = LinearRegression(featuresCol = 'features', labelCol='Total_Cup_Points', maxIter=10)\n",
        "lr_model = lr.fit(train_taste)\n",
        "print(\"Coefficients: \" + str(lr_model.coefficients[:5])+ \"\\n\"+ str(lr_model.coefficients[5:]))\n",
        "print(\"Intercept: \" + str(lr_model.intercept))"
      ],
      "metadata": {
        "id": "U_W6uJA5dAgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All taste characteristics are scored out of 10 and the characteristic Total_Cup_Points is their aggregation.\n",
        "\n",
        "We can therefore assume that all taste characteristics have the same importance.\n",
        "\n",
        "By running a linear regression model, we can see that all the features have a close importance, but some of them have a slightly greater impact on the model.\n",
        "\n",
        "We can then make 2 assumptions:\n",
        "\n",
        "1st assumption: because of the correlation between the characteristics, some of them are considered more important because when their score is high, it increases the score of the others.\n",
        "\n",
        "2nd assumption: some characteristics have a high variance and the others a low one, those with a high variance will have more impact on the final score.\n",
        "\n",
        "The slightly more important characteristics seem to be body and balance, but they are not very significant."
      ],
      "metadata": {
        "id": "lMY75yCsVy1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on the train data \n",
        "print(\"MSE: \", lr_model.summary.meanSquaredError)\n",
        "print(\"MAE: \", lr_model.summary.meanAbsoluteError)\n",
        "print(\"R-squared: \", lr_model.summary.r2)"
      ],
      "metadata": {
        "id": "qNP-xIbP4uxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predicting and evaluating the model"
      ],
      "metadata": {
        "id": "Uzmm7PSCealT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_predictions = lr_model.transform(test_taste)\n",
        "lr_predictions.select(\"prediction\",\"Total_Cup_Points\",\"features\").show(5)"
      ],
      "metadata": {
        "id": "dvHB_lD0eiD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on test data \n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
        "                 labelCol=\"Total_Cup_Points\")\n",
        "\n",
        "print(\"MAE on test data = %g\" % lr_evaluator.evaluate(lr_predictions, {lr_evaluator.metricName: \"mae\"}))\n",
        "print(\"MSE on test data = %g\" % lr_evaluator.evaluate(lr_predictions, {lr_evaluator.metricName: \"mse\"}))\n",
        "print(\"R-squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions, {lr_evaluator.metricName: \"r2\"}))\n"
      ],
      "metadata": {
        "id": "eKrAhrZIfB3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Th R squared of our model is very high that means that the variation of tastes variables explains very well the note given for a coffee. In the other hand, our model fits quite well the data."
      ],
      "metadata": {
        "id": "PA6mxGR3fSIK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### In the graph below, we can see that the prediction values curve is almost superimposed on the original values curve"
      ],
      "metadata": {
        "id": "y05K935YztVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_ax = range(0,lr_predictions.count())\n",
        "y_pred = lr_predictions.select(\"prediction\").collect()\n",
        "y_orig = lr_predictions.select(\"Total_Cup_Points\").collect()"
      ],
      "metadata": {
        "id": "2F94FxaTyRqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plt.plot(x_ax, y_orig, label=\"Total_Cup_Points\")\n",
        "plt.plot(x_ax, y_pred, label=\"prediction\")\n",
        "plt.title(\"Real notes on the test data and the predicted ones\")\n",
        "plt.xlabel('X-axis')\n",
        "plt.ylabel('Y-axis')\n",
        "plt.legend(loc='best',fancybox=True, shadow=True)\n",
        "plt.grid(True)\n",
        "plt.show()  \n",
        " \n"
      ],
      "metadata": {
        "id": "NU5WAyvJfpwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variety and scoring relation"
      ],
      "metadata": {
        "id": "_0VWZKVTY8kL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### For this one we will just use visualizations to see the variation of coffee grade according to the variet."
      ],
      "metadata": {
        "id": "xNQaoih5f07i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "var = df_arabica.groupBy(\"Variety\").mean(\"Total_Cup_Points\")\n",
        "var.show()"
      ],
      "metadata": {
        "id": "gBcQiNAFZAEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "px.bar(var.toPandas(), x='Variety', y='avg(Total_Cup_Points)')"
      ],
      "metadata": {
        "id": "UP7tnDvCJz5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "sns.lineplot(x=\"Variety\", y='avg(Total_Cup_Points)', data=var.toPandas()).set_title('Variation of the coffee grade according to the variety')"
      ],
      "metadata": {
        "id": "rneAG8EfGVVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Not a very convincing plot, the Total_Cup_Points are very close (in the 80-90 range) and variety doesn't seem to matter that much"
      ],
      "metadata": {
        "id": "YV1Wp3-qkD9y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Farm and scoring relation"
      ],
      "metadata": {
        "id": "kcQRu2pwZAPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "farmr = df_arabica.groupBy(\"Farm_name\").mean(\"Total_Cup_Points\")\n",
        "farmr.show()"
      ],
      "metadata": {
        "id": "a1qhNoYzmJRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "px.bar(farmr.toPandas().sort_values(by=['avg(Total_Cup_Points)']), x='Farm_name', y='avg(Total_Cup_Points)')"
      ],
      "metadata": {
        "id": "Uk1abBQCmakC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Once again the plot is uninteresting, we can try to scale Total_Cup_Points to see more clearly (and with it let's do it on the tastes column since they suffer the same problem)"
      ],
      "metadata": {
        "id": "2_p6cn1BoM3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalizing"
      ],
      "metadata": {
        "id": "I0paRqsloYsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import MinMaxScaler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import udf"
      ],
      "metadata": {
        "id": "eFnnRJcooas7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unlist = udf(lambda x: round(float(list(x)[0]),3), DoubleType())\n",
        "df_arabica_scaled = df_arabica\n",
        "\n",
        "for i in [\"Aroma\",\"Flavor\",\"Aftertaste\",\\\n",
        "                   \"Acidity\",\"Body\",\"Balance\",\"Total_Cup_Points\"]:\n",
        "    # VectorAssembler Transformation - Converting column to vector type\n",
        "    assembler = VectorAssembler(inputCols=[i],outputCol=i+\"_Vect\")\n",
        "\n",
        "    # MinMaxScaler Transformation\n",
        "    scaler = MinMaxScaler(inputCol=i+\"_Vect\", outputCol=i+\"_Scaled\")\n",
        "\n",
        "    # Pipeline of VectorAssembler and MinMaxScaler\n",
        "    pipeline = Pipeline(stages=[assembler, scaler])\n",
        "\n",
        "    # Fitting pipeline on dataframe\n",
        "    df_arabica_scaled = pipeline.fit(df_arabica_scaled).transform(df_arabica_scaled).withColumn(i+\"_Scaled\", unlist(i+\"_Scaled\")).drop(i+\"_Vect\")\n",
        "\n",
        "print(\"After Scaling :\")\n",
        "df_arabica_scaled.show(5)"
      ],
      "metadata": {
        "id": "6jyses5EoLxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "source : https://stackoverflow.com/questions/40337744/scalenormalise-a-column-in-spark-dataframe-pyspark"
      ],
      "metadata": {
        "id": "QrPiRf-nqVMh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Back to Exploration"
      ],
      "metadata": {
        "id": "rVBT1T4vsNub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's try again the previous plots"
      ],
      "metadata": {
        "id": "Gb_aMJwxsZsr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variety"
      ],
      "metadata": {
        "id": "-qD3fjJJspzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "var = df_arabica_scaled.groupBy(\"Variety\").mean(\"Total_Cup_Points_Scaled\")\n",
        "px.bar(var.toPandas(), x='Variety', y='avg(Total_Cup_Points_Scaled)')"
      ],
      "metadata": {
        "id": "tb6jmXElsePZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "sns.lineplot(x=\"Variety\", y='avg(Total_Cup_Points_Scaled)', data=var.toPandas()).set_title('Variation of the coffee grade according to the variety')"
      ],
      "metadata": {
        "id": "EUAEFdcwJlC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TOOOOOOOOOOOOOOOOOOOO DOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO"
      ],
      "metadata": {
        "id": "mAHj_A4ItRGm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Farms"
      ],
      "metadata": {
        "id": "KQ_GJ3HWsnqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "farmr = df_arabica_scaled.groupBy(\"Farm_name\").mean(\"Total_Cup_Points_Scaled\")\n",
        "px.bar(farmr.toPandas().sort_values(by=['avg(Total_Cup_Points_Scaled)']), x='Farm_name', y='avg(Total_Cup_Points_Scaled)')"
      ],
      "metadata": {
        "id": "yK10qWfnshxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Way clearer"
      ],
      "metadata": {
        "id": "P6wQhh1Gs9N_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We can notice that farms are more important offer more distinction than variety, we will see furthermore"
      ],
      "metadata": {
        "id": "naGA7j8YwcFt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## From now on we will use only the scaled version"
      ],
      "metadata": {
        "id": "CyhKDPdEtYwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_arabica = df_arabica_scaled\n",
        "\n",
        "cols = (\"Aroma\",\"Flavor\",\"Aftertaste\",\\\n",
        "                   \"Acidity\",\"Body\",\"Balance\",\"Total_Cup_Points\")\n",
        "df_arabica = df_arabica.drop(*cols)\n",
        "for i in [\"Aroma\",\"Flavor\",\"Aftertaste\",\\\n",
        "                   \"Acidity\",\"Body\",\"Balance\",\"Total_Cup_Points\"]:\n",
        "                   df_arabica = df_arabica.withColumnRenamed(i+\"_Scaled\", i)\n",
        "\n",
        "df_arabica.show()\n",
        "df_arabica.printSchema()"
      ],
      "metadata": {
        "id": "0uzHwEqgtfng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Farms all of fame and their strengths"
      ],
      "metadata": {
        "id": "QIjCRSP9mJb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Since farms seem interesting we can plot radar plots of the taste they offer (meaned).\n",
        "#### We will focus on the most renowed farms"
      ],
      "metadata": {
        "id": "JvBejSPCwtd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import desc\n",
        "hof = df_arabica.groupBy(\"Farm_name\").mean(\"Total_Cup_Points\").sort(desc(\"avg(Total_Cup_Points)\"))\n",
        "hof.show(6)"
      ],
      "metadata": {
        "id": "kZ_f6k13ZGM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " li = hof.select(\"Farm_name\").take(3)"
      ],
      "metadata": {
        "id": "sw2KEvvCWyrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "li = [\"fazenda kaquend\",\"fazenda recreio\",\"sitío são geraldo\",\"sitio claro\",\"fazenda grota funda\"]"
      ],
      "metadata": {
        "id": "0T9A7vnyWzqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hof = df_arabica.filter(df_arabica.Farm_name.isin(li))\n",
        "df_hof.show()"
      ],
      "metadata": {
        "id": "Vn3yJlmCW4Ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "from pyspark.sql.functions import mean\n",
        "\n",
        "categories = [\"Aroma\",\"Flavor\",\"Aftertaste\",\\\n",
        "                   \"Acidity\",\"Body\",\"Balance\"]\n",
        "\n",
        "df_hof_pd = df_hof.select([\"Farm_name\",\"Aroma\",\"Flavor\",\"Aftertaste\",\\\n",
        "                   \"Acidity\",\"Body\",\"Balance\"]).toPandas().set_index('Farm_name')\n",
        "\n",
        "\n",
        "df_mean_pd = df_arabica.select(mean(\"Aroma\"), mean(\"Flavor\")\\\n",
        "                               , mean(\"Aftertaste\"), mean(\"Acidity\"), mean(\"Body\"), mean(\"Balance\"))\\\n",
        "                               .toPandas()\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "for g in df_hof_pd.index:\n",
        "    fig.add_trace(go.Scatterpolar(\n",
        "        r = df_hof_pd.loc[g].values,\n",
        "        theta = categories,\n",
        "        fill = \"toself\",\n",
        "        name = f'{g}'\n",
        "    ))\n",
        "\n",
        "\n",
        "fig.add_trace(go.Scatterpolar(\n",
        "        r = df_mean_pd.values,\n",
        "        theta = categories,\n",
        "        fill = \"toself\",\n",
        "        name = f'MEAN'\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "  polar=dict(\n",
        "    radialaxis=dict(\n",
        "      visible=False,\n",
        "      range=[0, 1]\n",
        "    )),\n",
        "  showlegend=False\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "TAukSS8o3I3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recommend a coffee a taste profile"
      ],
      "metadata": {
        "id": "K7LmKNycSKaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "\n",
        "\n",
        "assembler = VectorAssembler(inputCols= categories,outputCol=\"features\")\n",
        "\n",
        "df_km= assembler.transform(df_arabica)\n",
        "\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "cluster_score =[]\n",
        "evaluator = ClusteringEvaluator(predictionCol='prediction', featuresCol='features', \\\n",
        "                                metricName='silhouette', distanceMeasure='squaredEuclidean')\n",
        "\n",
        "nbmaxclust = 20 \n",
        "for nbclust in range(2,nbmaxclust):\n",
        "    \n",
        "    kmeans=KMeans(featuresCol='features', k=nbclust)\n",
        "    \n",
        "    kmeans=kmeans.fit(df_km).transform(df_km)\n",
        "    \n",
        "    score = evaluator.evaluate(kmeans)\n",
        "    \n",
        "    cluster_score.append(score)\n",
        "    \n",
        "    print(\"Cluster Score:\",score)\n"
      ],
      "metadata": {
        "id": "suKgK4caSM-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_to_plot = pd.DataFrame({'score': cluster_score, 'nbclust': range(2,nbmaxclust)})"
      ],
      "metadata": {
        "id": "6pBJbeb3SPpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.line(score_to_plot, x=\"nbclust\", y=\"score\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "pL0R-TcnSSBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The optimal number of clusters seems to be 10"
      ],
      "metadata": {
        "id": "nBhgRUdIVl8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nbo_clust = score_to_plot.sort_values('score',ascending=False).head(1)\n",
        "nbo_clust"
      ],
      "metadata": {
        "id": "rKGUOG5sSUiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans=KMeans(featuresCol='features', k=int(nbo_clust.nbclust))\n",
        "    \n",
        "kmeans=kmeans.fit(df_km).transform(df_km)"
      ],
      "metadata": {
        "id": "cQvQO0oESakE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "friend = spark.createDataFrame(\n",
        "    [(6,6,10,5,10,6)],\n",
        "    [\"Aroma\",\"Flavor\",\"Aftertaste\",\"Acidity\",\"Body\", \"Balance\"])"
      ],
      "metadata": {
        "id": "6cPfoXBjSeen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans.transform(friend)"
      ],
      "metadata": {
        "id": "FX2s49WOShZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The climat of Brazil over the last 22 years"
      ],
      "metadata": {
        "id": "yPOzJ3zuhk1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"/content/gdrive/MyDrive/donmass_proj/\"\n",
        "# Brazil's weather stations\n",
        "stations_brazil = pd.read_csv(url+\"stations_.csv\",sep=\";\")"
      ],
      "metadata": {
        "id": "3VdALmugdIPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stations_brazil.head()"
      ],
      "metadata": {
        "id": "2173kROUdRUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stations_brazil.info()"
      ],
      "metadata": {
        "id": "z0WhGigNe0ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert pandas dataframe to spark dataFrame\n",
        "df_stations = pandas_to_spark(stations_brazil) "
      ],
      "metadata": {
        "id": "Oth59bdg54zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_stations.printSchema()"
      ],
      "metadata": {
        "id": "qme4sstL59Yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import plotly.graph_objects as go\n",
        "fig = px.scatter_mapbox(df_stations.toPandas(), lat=\"lat\", lon=\"lon\", hover_name=\"id_station\", hover_data=[\"lvl\",], color_discrete_sequence=[\"blue\"], zoom=3, height=300)\n",
        "fig.add_trace(go.Scatter(x=df_arabica.toPandas()['lat'], y=df_arabica.toPandas()['long'],\n",
        "                    mode='markers',\n",
        "                    name='farms',\n",
        "                    marker_color='rgba(152, 0, 0, .8)'))\n",
        "fig.update_layout(mapbox_style=\"open-street-map\")\n",
        "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "BzeXTXlv46OR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  As most of the farms are located in the southest of brazil, we onely focus on one and analysis its climate changes, this station's id= A530, ie Poços de Caldas "
      ],
      "metadata": {
        "id": "E7nXTo9y6vDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Brazil's weather 22 year history\n",
        "weather_brazil = pd.read_csv(url+\"weather_sum_all.csv\")"
      ],
      "metadata": {
        "id": "D3xg7iOa-UHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weather_brazil.head()"
      ],
      "metadata": {
        "id": "ufii6Zh-vyJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weather_brazil.info()"
      ],
      "metadata": {
        "id": "baVgHE252UVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fill the missing values with the revious one\n",
        "weather_brazil = weather_brazil.fillna(method='ffill')"
      ],
      "metadata": {
        "id": "jpAL0zhyO11z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# new datetime column \n",
        "weather_brazil['datetime'] = pd.to_datetime(weather_brazil['DATA (YYYY-MM-DD)'], format='%Y-%m-%d %H:%M:%S')"
      ],
      "metadata": {
        "id": "H8q5Oahp2uBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a year column\n",
        "weather_brazil['year'] = weather_brazil['datetime'].dt.year"
      ],
      "metadata": {
        "id": "xoPYAGdubfvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert pandas to spark\n",
        "df_weather = pandas_to_spark(weather_brazil)"
      ],
      "metadata": {
        "id": "7e2RjY2ZClEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select the station needed\n",
        "df_southeast = df_weather.filter(df_weather[\"ESTACAO\"]=='A530')\n",
        "df_southeast .show(100, 100)"
      ],
      "metadata": {
        "id": "GLvD2xvsCkHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iuzm6XZpcFUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MSRBUuefClgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We create a Year column"
      ],
      "metadata": {
        "id": "OsKH17QNWSUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, to_timestamp, year\n",
        "# create a year column \n",
        "df_weather = df_weather.withColumn('year', year(col('date')))"
      ],
      "metadata": {
        "id": "lgW2nIFQVb0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_weather.show(10)"
      ],
      "metadata": {
        "id": "WTx8MiHjWC26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lKoXjdfPa2Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TqKYJYBtYuvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "05pTdtPfYu2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rasterio\n",
        "!pip install pyproj"
      ],
      "metadata": {
        "id": "Tp5GwEMuAH3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import rasterio\n",
        "import rasterio.plot\n",
        "import pyproj\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "jfkKHYbkwesg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "f, axarr = plt.subplots(2,2)\n",
        "\n",
        "\n",
        "for i,itif in enumerate( [\"avgtemp.tif\", \"prec.tif\", \"srac.tif\", \"vapr.tif\"]) :\n",
        "  with rasterio.open(\"/content/gdrive/MyDrive/donmass_proj/\"+itif) as src:\n",
        "    img = src.read(1)\n",
        "    #print(src.crs)\n",
        "    if(i<2):\n",
        "      a = 0\n",
        "    else:\n",
        "      a = 1\n",
        "      i -=2 \n",
        "    pos = src.index(-52.196283130028455,-13.574677798648072) #long lat \n",
        "    axarr[a][i].imshow(img[pos[0]-2200:pos[0]+2200,pos[1]-2200:pos[1]+2200]) "
      ],
      "metadata": {
        "id": "rV2Pwr6Kvxs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We wanted to run a model on our rasters to predict the right atmospheric conditions to grow coffee. This could be further used to predict on a dataset containing the same features, in the case of our study, on a predicted climate influenced by global warming.\n",
        "However after much struggle on rasters, and seeing that these struggle were only on the preprocessing part, we realized that we won't be able to implement a species distribution modelling because of the aforementioned problem, and the extreme specificity of these models, that we never encountered, we decided to go to something closer to our knowledge. \n",
        "We got the idea of bringing the problem to a image classification task, predicting the possible presence of coffee from image composed of all bands of the rasters. We sampled some (100,100) squares on fields growing coffe and on some close proximity of the fields.\n",
        "Unfortunately we encountered two main problems making the model completely bad:\n",
        "  - We need to sampled way more image, which is time consuming. Having such a low number of image makes the training and the ensuing predictions close to a coinflip, while being overly prone to over-fitting.\n",
        "  - Pre-trained models are based on 3 bands images, making them hard (with my limited skills) to adapt them to our 12 bands images. I had to create my own poor CNN with no transfer learning which makes it very inefficient.\n",
        "\n",
        "#### Anyway here's the (poorly written) code "
      ],
      "metadata": {
        "id": "GpSWtGSBTTJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_farms_pd = "
      ],
      "metadata": {
        "id": "P35w0uCv6c2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir /content/drive/MyDrive/donmass_proj/images/coffee/\n",
        "! mkdir /content/drive/MyDrive/donmass_proj/images/nocoffee/"
      ],
      "metadata": {
        "id": "vOS_8g6rN3Ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,itif in enumerate( [\"avgtemp.tif\", \"prec.tif\", \"srac.tif\", \"vapr.tif\"]) :\n",
        "  print(itif)\n",
        "  with rasterio.open(\"/content/drive/MyDrive/donmass_proj/\"+itif) as src:\n",
        "    img = src.read(1)\n",
        "    print(img.shape)\n",
        "    #center = src.index(-52.196283130028455,-13.574677798648072) #long lat \n",
        "    for index, row in df_farms_pd.iterrows():\n",
        "      farm_name = row[\"Farm_name\"]\n",
        "      pos = src.index(row[\"long\"], row[\"lat\"])\n",
        "      im = img[pos[0]-50:pos[0]+50,pos[1]-50:pos[1]+50]\n",
        "      matplotlib.image.imsave(\"/content/drive/MyDrive/donmass_proj/images/coffee/\"  +str(i) + farm_name + \".png\", im)\n",
        "      im = img[pos[0]+100-50:pos[0]+100+50,pos[1]+100-50:pos[1]+100+50]\n",
        "      matplotlib.image.imsave(\"/content/drive/MyDrive/donmass_proj/images/nocoffee/\"+str(i) + farm_name + \".png\", im)\n",
        "      "
      ],
      "metadata": {
        "id": "ATlJ9RudThe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#! rm -r /content/drive/MyDrive/donmass_proj/images/nocoffee/\n",
        "#! rm -r /content/drive/MyDrive/donmass_proj/images/coffee/"
      ],
      "metadata": {
        "id": "xZFiPqk_Tjsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merging them into one N-Dimension Image"
      ],
      "metadata": {
        "id": "uk9d-9YtTpIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir /content/drive/MyDrive/donmass_proj/images/coffee/inone\n",
        "! mkdir /content/drive/MyDrive/donmass_proj/images/nocoffee/inone"
      ],
      "metadata": {
        "id": "_4LWoRbiTpzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from os import listdir"
      ],
      "metadata": {
        "id": "Q9vhQE-OTs0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#! rm -r /content/drive/MyDrive/donmass_proj/images/nocoffee/inone\n",
        "#! rm -r /content/drive/MyDrive/donmass_proj/images/coffee/inone"
      ],
      "metadata": {
        "id": "Iscn3BjRTuqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allcoffee = []\n",
        "for index, row in df_farms_pd.iterrows():\n",
        "  farm_name = row[\"Farm_name\"]\n",
        "  allin = [Image.open(\"/content/drive/MyDrive/donmass_proj/images/coffee/\" + str(i) + farm_name + \".png\") for i in range(4)]\n",
        "  allinone = []\n",
        "  #print(allin) #Nos 4 geotifs en img\n",
        "  for image in allin:\n",
        "    r, g, b,t  = image.split() #4 bandes par img dont 1 inutile\n",
        "    allinone.append(np.array(r))\n",
        "    allinone.append(np.array(g))\n",
        "    allinone.append(np.array(b))\n",
        "    \n",
        "  allcoffee.append(np.array(allinone).reshape((100,100,12)))\n",
        "  #print(allinone)"
      ],
      "metadata": {
        "id": "8d0epVAETwZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allnocoffee = []\n",
        "for index, row in df_farms_pd.iterrows():\n",
        "  farm_name = row[\"Farm_name\"]\n",
        "  allin = [Image.open(\"/content/drive/MyDrive/donmass_proj/images/nocoffee/\" + str(i) + farm_name + \".png\") for i in range(4)]\n",
        "  allinone = []\n",
        "  #print(allin) #Nos 4 geotifs en img\n",
        "  for image in allin:\n",
        "    r, g, b,t  = image.split() #4 bandes par img dont 1 inutile\n",
        "    allinone.append(np.array(r))\n",
        "    allinone.append(np.array(g))\n",
        "    allinone.append(np.array(b))\n",
        "    \n",
        "  allnocoffee.append(np.array(allinone).reshape((100,100,12)))"
      ],
      "metadata": {
        "id": "Ql7e_M7wTyX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.array(allnocoffee).shape)\n",
        "print(np.array(allcoffee).shape)"
      ],
      "metadata": {
        "id": "W8OSi1vIT4Yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allnocoffee = [(0,np.array(allnocoffee)[i]) for i in range(len(np.array(allnocoffee)))]\n",
        "allcoffee = [(1,np.array(allcoffee)[i]) for i in range(len((allcoffee)))]"
      ],
      "metadata": {
        "id": "dm0HA5YMT0OJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all = allcoffee+allnocoffee"
      ],
      "metadata": {
        "id": "6Qn9f2SKT3Mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all)"
      ],
      "metadata": {
        "id": "Wwsj-8qXT8op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Should train/test split but low number of images let's see how it trains"
      ],
      "metadata": {
        "id": "QvQOjPy5T_hB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN"
      ],
      "metadata": {
        "id": "zu17e44bUF4C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models\n"
      ],
      "metadata": {
        "id": "ueqKpgJwUGwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "Bp268-i2UMTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input((100,100,12)),\n",
        "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    tf.keras.layers.RandomContrast(0.2),\n",
        "    tf.keras.layers.RandomBrightness([-0.5,0.5]),\n",
        "    tf.keras.layers.RandomRotation(0.2),\n",
        "    tf.keras.layers.Conv2D(32, kernel_size=1, padding='valid', activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "    tf.keras.layers.Conv2D(48, kernel_size=1, padding='valid', activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(2, activation='sigmoid')"
      ],
      "metadata": {
        "id": "PneI0SOnUOzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(shape=(100, 100, 12)),\n",
        "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    tf.keras.layers.RandomContrast(0.2),\n",
        "    tf.keras.layers.RandomBrightness([-0.5,0.5]),\n",
        "    tf.keras.layers.RandomRotation(0.2),\n",
        "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=1),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Activation('relu'),\n",
        "    tf.keras.layers.Conv2D(filters=64, kernel_size=1, strides=1),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Activation('relu'),\n",
        "    tf.keras.layers.Conv2D(filters=64, kernel_size=1, strides=1),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.GlobalMaxPooling2D(),\n",
        "    tf.keras.layers.Dense(2, activation='sigmoid')\n",
        "])\n"
      ],
      "metadata": {
        "id": "rUkUmrSJURFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Generator"
      ],
      "metadata": {
        "id": "hgU0KmvlUTSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### there might be a problem here also"
      ],
      "metadata": {
        "id": "z8ee4SavUWcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, size, data, batch_size=32,  dataaug=True, shuffle=True):\n",
        "        self.batch_size = batch_size\n",
        "        self.data = data    \n",
        "        self.dataaug=dataaug\n",
        "        self.indices = range(len(data[:][:]))\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.size = size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index = self.index[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        keys = [self.indices[k] for k in index]\n",
        "        return self.__get_data(keys)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.index = np.arange(len(self.indices))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.index)\n",
        "\n",
        "            \n",
        "    def __get_data(self, keys):        \n",
        "        X = np.zeros((self.batch_size, self.size, self.size, 12))\n",
        "        y = np.zeros((self.batch_size,1))\n",
        "\n",
        "        for i, id in enumerate(keys):\n",
        "          X[i] = self.data[id][1]\n",
        "          y[i] = self.data[id][0]\n",
        "\n",
        "        return X,y\n",
        "                \n"
      ],
      "metadata": {
        "id": "W-XMB694T-4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "EPOCHS = 10\n",
        "IMN_EPOCHS = 1000\n",
        "IMG_SIZE = 100"
      ],
      "metadata": {
        "id": "ag3XnJjQUZSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = DataGenerator(data = all, size = IMG_SIZE, batch_size = BATCH_SIZE)"
      ],
      "metadata": {
        "id": "g3-evctoUc-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trainning "
      ],
      "metadata": {
        "id": "IHlbJds7UfUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2.compile(optimizer='adam', loss = tf.keras.losses.BinaryCrossentropy(), metrics = \"accuracy\")"
      ],
      "metadata": {
        "id": "87l7eTrqUhWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model2.fit(train, epochs = EPOCHS)"
      ],
      "metadata": {
        "id": "AI3QvfATUjFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "eDmYw4KGUlRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])"
      ],
      "metadata": {
        "id": "HcgjU69BUnHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.predict(DataGenerator(data = allcoffee,size = IMG_SIZE,batch_size=1))"
      ],
      "metadata": {
        "id": "gFHJpFQRUo9w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}